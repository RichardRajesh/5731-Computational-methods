{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RichardRajesh/5731-Computational-methods/blob/main/Rajesh_Bodas_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Wednesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWLqayKIz9pl",
        "outputId": "da1615f0-ac5d-4e9a-dc4e-3461c97bcae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def Movie_Reviews(movieUrls, No_Reviews, outputDataFile):\n",
        "    movieReviewsData = []\n",
        "    countOfReviews = 0\n",
        "\n",
        "    for movieUrl in movieUrls:\n",
        "        webPageNum = 1\n",
        "\n",
        "        while countOfReviews < No_Reviews:\n",
        "            webpageUrl = f\"{movieUrl}&page={webPageNum}\"\n",
        "\n",
        "            webResponse = requests.get(webpageUrl)\n",
        "            if webResponse.status_code != 200:\n",
        "                print(f\"Failed to retrieve page data {webPageNum} from {movieUrl}. Exiting.\")\n",
        "                break\n",
        "\n",
        "            soupData = BeautifulSoup(webResponse.text, 'html.parser')\n",
        "            containersReviews = soupData.find_all('div', class_='lister-item-content')\n",
        "\n",
        "            if not containersReviews:\n",
        "                print(f\"No reviews were found on page {webPageNum} from {movieUrl}. Exiting.\")\n",
        "                break\n",
        "\n",
        "            for reviewContent in containersReviews:\n",
        "                reviewText = reviewContent.find('div', class_='text').get_text()\n",
        "                reviewUsername = reviewContent.find('span', class_='display-name-link').get_text()\n",
        "                reviewPostedDate = reviewContent.find('span', class_='review-date').get_text()\n",
        "\n",
        "                movieReviewsData.append([reviewUsername, reviewPostedDate, reviewText])\n",
        "                countOfReviews += 1\n",
        "\n",
        "                if countOfReviews >= No_Reviews:\n",
        "                    break\n",
        "\n",
        "            webPageNum += 1\n",
        "\n",
        "    if movieReviewsData:\n",
        "        with open(outputDataFile, 'w', newline='', encoding='utf-8') as csvFile:\n",
        "            csvWriter = csv.writer(csvFile)\n",
        "            csvWriter.writerow(['Username', 'Review Date', 'Review Text'])  # Header\n",
        "            csvWriter.writerows(movieReviewsData)\n",
        "\n",
        "        print(f\"{len(movieReviewsData)}Movie reviews have been successfully scraped from the website and saved to '{outputDataFile}'.\")\n",
        "    else:\n",
        "        print(\"No reviews were found on the pages.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    movieUrls = [\n",
        "        'https://www.imdb.com/title/tt23289160/reviews/?ref_=tt_ov_rt',\n",
        "        'https://www.imdb.com/title/tt23849204/reviews/?ref_=tt_ov_rt'\n",
        "    ]\n",
        "    No_Reviews = 1000\n",
        "    outputDataFile = 'combined_movie_reviews.csv'\n",
        "\n",
        "    Movie_Reviews(movieUrls, No_Reviews, outputDataFile)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOvKERCh0qGZ",
        "outputId": "b9bcf54f-4554-4390-e1aa-c6e852d1641e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000Movie reviews have been successfully scraped from the website and saved to 'combined_movie_reviews.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n"
      ],
      "metadata": {
        "id": "LM2X0HnV6A3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def Clean_Text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stopWords = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    filteredText = [word for word in words if word.lower() not in stopWords]\n",
        "    text = ' '.join(filteredText)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmedText = [stemmer.stem(word) for word in text.split()]\n",
        "    text = ' '.join(stemmedText)\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatizedText = [lemmatizer.lemmatize(word) for word in text.split()]\n",
        "    text = ' '.join(lemmatizedText)\n",
        "\n",
        "    return text\n",
        "\n",
        "def Clean_And_Save_Reviews(inputCsvFile, outputCsvFile):\n",
        "    with open(inputCsvFile, 'r', encoding='utf-8') as inFile:\n",
        "        reader = csv.reader(inFile)\n",
        "        header = next(reader)\n",
        "        header.append('Clean Text')\n",
        "\n",
        "        rows = []\n",
        "        for row in reader:\n",
        "            reviewText = row[2]\n",
        "            cleanTextReview = Clean_Text(reviewText)\n",
        "            row.append(cleanTextReview)\n",
        "            rows.append(row)\n",
        "\n",
        "\n",
        "    with open(outputCsvFile, 'w', newline='', encoding='utf-8') as outFile:\n",
        "        writer = csv.writer(outFile)\n",
        "        writer.writerow(header)\n",
        "        writer.writerows(rows)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    inputCsvFile = 'combined_movie_reviews.csv'\n",
        "    outputCsvFile = 'cleaned_movie_reviews.csv'\n",
        "\n",
        "    Clean_And_Save_Reviews(inputCsvFile, outputCsvFile)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pKsEUW01SuE",
        "outputId": "ca31ae1c-83bb-4272-947b-8130e872ff8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def Clean_Text(Text_Data):\n",
        "    Text_Data = re.sub(r'[^a-zA-Z\\s]', '', Text_Data)\n",
        "    Text_Data = re.sub(r'\\d+', '', Text_Data)\n",
        "    # Remove stopwords\n",
        "    Stop_Words = set(stopwords.words('english'))\n",
        "    Words_Data = Text_Data.split()\n",
        "    Filtered_Text_Data = [word for word in Words_Data if word.lower() not in Stop_Words]\n",
        "    Text_Data = ' '.join(Filtered_Text_Data)\n",
        "\n",
        "    # Lowercase all texts\n",
        "    Text_Data = Text_Data.lower()\n",
        "\n",
        "    # Stemming\n",
        "    Stemmer = PorterStemmer()\n",
        "    Stemmed_Text_Data = [Stemmer.stem(word) for word in Text_Data.split()]\n",
        "    Text_Data = ' '.join(Stemmed_Text_Data)\n",
        "\n",
        "    # Lemmatization\n",
        "    Lemmatizer = WordNetLemmatizer()\n",
        "    Lemmatized_Text_Data = [Lemmatizer.lemmatize(word) for word in Text_Data.split()]\n",
        "    Text_Data = ' '.join(Lemmatized_Text_Data)\n",
        "\n",
        "    return Text_Data\n",
        "\n",
        "def Clean_And_Save_Reviews(Input_Text_File, Output_Text_File):\n",
        "    with open(Input_Text_File, 'r', encoding='utf-8') as InputFile:\n",
        "        Reader = csv.reader(InputFile)\n",
        "        Read_Header = next(Reader)  # Read the header\n",
        "\n",
        "        # Append 'Clean Text' to the header\n",
        "        Read_Header.append('Clean Text')\n",
        "\n",
        "        Rows = []\n",
        "        for Line in Reader:\n",
        "            Review_Text = Line[2]  # Assuming the review text is in the third column\n",
        "            Cleaned_Text_Review = Clean_Text(Review_Text)\n",
        "            Line.append(Cleaned_Text_Review)\n",
        "            Rows.append(Line)\n",
        "\n",
        "    # Write the cleaned data to a new CSV file\n",
        "    with open(Output_Text_File, 'w', newline='', encoding='utf-8') as OutputFile:\n",
        "        NewFileWriter = csv.writer(OutputFile)\n",
        "        NewFileWriter.writerow(Read_Header)  # Write the header\n",
        "        NewFileWriter.writerows(Rows)\n",
        "\n",
        "    print(f\"Cleaned data is stored into '{Output_Text_File}'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    Input_Text_File = 'combined_movie_reviews.csv'\n",
        "    Output_Text_File = 'cleaned_movie_reviews.csv'\n",
        "\n",
        "    Clean_And_Save_Reviews(Input_Text_File, Output_Text_File)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRScVM4R7v9_",
        "outputId": "559b020a-9371-4b5a-b065-6559a22b3ad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data is stored into 'cleaned_movie_reviews.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Write your code here\n",
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_text_data = 'combined_movie_reviews.csv'\n",
        "    output_text_data = 'cleaned_movie_reviews.csv'\n",
        "\n",
        "    #cleaned_and_saved_reviewed(input_text_data, output_text_data)\n",
        "\n",
        "    # Read the cleaned text data\n",
        "    df = pd.read_csv(output_text_data)\n",
        "\n",
        "    # (1) Parts of Speech (POS) Tagging\n",
        "    nouns_per_count = 0\n",
        "    verbs_count = 0\n",
        "    adj_counts = 0\n",
        "    adv_counts = 0\n",
        "\n",
        "    for text_data in df['Clean Text']:\n",
        "        docs = nlp(text_data)\n",
        "        for token in docs:\n",
        "            if token.pos_ == \"NOUN\":\n",
        "                nouns_per_count += 1\n",
        "            elif token.pos_ == \"VERB\":\n",
        "                verbs_count += 1\n",
        "            elif token.pos_ == \"ADJ\":\n",
        "                adj_counts += 1\n",
        "            elif token.pos_ == \"ADV\":\n",
        "                adv_counts += 1\n",
        "\n",
        "    print(f\"Noun Count: {nouns_per_count}\")\n",
        "    print(f\"Verb Count: {verbs_count}\")\n",
        "    print(f\"Adjective Count: {adj_counts}\")\n",
        "    print(f\"Adverb Count: {adv_counts}\")\n",
        "\n",
        "    # (2) Constituency Parsing and Dependency Parsing (using one sentence as an example)\n",
        "    sampled_data_text = df['Clean Text'].iloc[0]  # Take the first sentence as an example\n",
        "\n",
        "    # Constituency Parsing Tree\n",
        "    sample_data_doc = nlp(sampled_data_text)\n",
        "    print(\"\\nConstituency Parsing Tree:\")\n",
        "    for token in sample_data_doc:\n",
        "        print(f\"{token.text} [{token.dep_}]\", end=\" -> \")\n",
        "    print()\n",
        "\n",
        "    # Dependency Parsing Tree\n",
        "    print(\"\\nDependency Parsing Tree:\")\n",
        "    for token in sample_data_doc:\n",
        "        print(f\"{token.text} [{token.head.text}]\", end=\" -> \")\n",
        "    print()\n",
        "\n",
        "    # (3) Named Entity Recognition\n",
        "    entities_values = {\n",
        "        \"PERSON\": 0,\n",
        "        \"ORG\": 0,\n",
        "        \"LOC\": 0,\n",
        "        \"PRODUCT\": 0,\n",
        "        \"DATE\": 0\n",
        "    }\n",
        "\n",
        "    for text_data in df['Clean Text']:\n",
        "        docs = nlp(text_data)\n",
        "        for ent in docs.ents:\n",
        "            if ent.label_ in entities_values:\n",
        "                entities_values[ent.label_] += 1\n",
        "\n",
        "    print(\"\\nNamed Entity Counts:\")\n",
        "    for entity, count in entities_values.items():\n",
        "        print(f\"{entity}:{count}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "US-3kN6CJ-sx",
        "outputId": "939d8688-786c-407e-8842-596ed9a30abe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noun Count: 52160\n",
            "Verb Count: 22240\n",
            "Adjective Count: 18480\n",
            "Adverb Count: 5040\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "godzilla [nsubj] -> minu [nsubj] -> one [nummod] -> ryunosuk [dobj] -> kamiki [compound] -> minami [compound] -> hamab [compound] -> sakura [compound] -> ando [compound] -> yuki [compound] -> yamada [compound] -> kuranosuk [compound] -> sasak [compound] -> hidetaka [compound] -> yaoshioka [compound] -> yuya [compound] -> endo [compound] -> saki [nmod] -> nakatani [compound] -> director [npadvmod] -> takashi [compound] -> yamazaki [compound] -> herculean [compound] -> effort [compound] -> reviv [compound] -> kaiju [compound] -> franchis [compound] -> effortlessli [compound] -> awesom [nsubj] -> absolut [ccomp] -> toptier [compound] -> filmmak [compound] -> origin [compound] -> stori [nmod] -> gigant [amod] -> monster [compound] -> show [compound] -> shore [compound] -> post [dobj] -> wwii [compound] -> japan [compound] -> engag [compound] -> fulltilt [compound] -> invas [compound] -> tokyo [compound] -> environ [nsubj] -> atyp [compound] -> destruct [compound] -> death [nmod] -> key [amod] -> film [compound] -> strength [compound] -> plot [nsubj] -> involv [conj] -> exkamikaz [compound] -> pilot [compound] -> kamiki [dobj] -> suffer [ROOT] -> arguabl [amod] -> cinema [compound] -> poignant [nmod] -> ptsd [nmod] -> love [compound] -> interest [compound] -> hamab [compound] -> makeshift [nmod] -> famili [compound] -> attempt [compound] -> surviv [compound] -> thread [compound] -> palpabl [compound] -> redempt [compound] -> narr [amod] -> lace [compound] -> surprisingli [compound] -> heartfelt [compound] -> emot [nmod] -> clean [amod] -> visual [amod] -> fx [compound] -> propel [compound] -> radioact [compound] -> creatur [compound] -> jawdroppingli [compound] -> astound [dobj] -> neat [advcl] -> extra [amod] -> spini [nmod] -> back [advmod] -> ignit [nmod] -> recharg [nmod] -> nuclear [compound] -> energi [compound] -> eyepoppingli [dobj] -> epic [advmod] -> one [nummod] -> year [npadvmod] -> best [amod] -> film [npadvmod] -> \n",
            "\n",
            "Dependency Parsing Tree:\n",
            "godzilla [minu] -> minu [suffer] -> one [ryunosuk] -> ryunosuk [minu] -> kamiki [sakura] -> minami [sakura] -> hamab [sakura] -> sakura [hidetaka] -> ando [hidetaka] -> yuki [hidetaka] -> yamada [hidetaka] -> kuranosuk [hidetaka] -> sasak [hidetaka] -> hidetaka [yuya] -> yaoshioka [yuya] -> yuya [director] -> endo [director] -> saki [director] -> nakatani [director] -> director [minu] -> takashi [herculean] -> yamazaki [herculean] -> herculean [effort] -> effort [reviv] -> reviv [awesom] -> kaiju [awesom] -> franchis [awesom] -> effortlessli [awesom] -> awesom [absolut] -> absolut [minu] -> toptier [origin] -> filmmak [origin] -> origin [stori] -> stori [post] -> gigant [post] -> monster [show] -> show [shore] -> shore [post] -> post [absolut] -> wwii [invas] -> japan [engag] -> engag [invas] -> fulltilt [invas] -> invas [environ] -> tokyo [environ] -> environ [atyp] -> atyp [plot] -> destruct [death] -> death [plot] -> key [plot] -> film [strength] -> strength [plot] -> plot [involv] -> involv [minu] -> exkamikaz [pilot] -> pilot [kamiki] -> kamiki [involv] -> suffer [suffer] -> arguabl [makeshift] -> cinema [poignant] -> poignant [interest] -> ptsd [interest] -> love [interest] -> interest [makeshift] -> hamab [makeshift] -> makeshift [attempt] -> famili [attempt] -> attempt [astound] -> surviv [thread] -> thread [redempt] -> palpabl [redempt] -> redempt [emot] -> narr [lace] -> lace [surprisingli] -> surprisingli [emot] -> heartfelt [emot] -> emot [jawdroppingli] -> clean [jawdroppingli] -> visual [jawdroppingli] -> fx [jawdroppingli] -> propel [radioact] -> radioact [jawdroppingli] -> creatur [jawdroppingli] -> jawdroppingli [astound] -> astound [suffer] -> neat [suffer] -> extra [spini] -> spini [ignit] -> back [spini] -> ignit [eyepoppingli] -> recharg [eyepoppingli] -> nuclear [energi] -> energi [eyepoppingli] -> eyepoppingli [neat] -> epic [suffer] -> one [year] -> year [best] -> best [film] -> film [suffer] -> \n",
            "\n",
            "Named Entity Counts:\n",
            "PERSON:2040\n",
            "ORG:1920\n",
            "LOC:0\n",
            "PRODUCT:120\n",
            "DATE:400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Constituency Parsing and Dependency Parsing:**\n",
        "\n",
        "While the dependency parsing tree shows the relationships as directed links between individual words, such as \"Godzilla\" being linked to \"minu\" through the relation \"suffer\" to illustrate their syntactic dependency, the constituency parsing tree structures the sentence hierarchically, indicating the relationships between different parts of speech. For example, \"Godzilla\" is the subject (\"nsubj\") and \"minu\" is the root."
      ],
      "metadata": {
        "id": "h8CW6P2jbCXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
        "from collections import Counter\n",
        "from nltk.parse import ChartParser\n",
        "import svgling\n",
        "\n",
        "# Load the spaCy model for Named Entity Recognition\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load your cleaned data from the CSV file\n",
        "df = pd.read_csv('cleaned_movie_reviews.csv')\n",
        "\n",
        "# (1) Parts of Speech (POS) Tagging\n",
        "# Tag Parts of Speech of each word in the text\n",
        "pos = [nltk.pos_tag(word_tokenize(sent)) for sent in df['Cleaned Text']]\n",
        "print(pos, end=\"\\n\\n\")\n",
        "\n",
        "# Calculate the total number of Noun (N), Verb (V), Adjective (Adj), and Adverb (Adv)\n",
        "counts = [Counter(tag for _, tag in tags) for tags in pos]\n",
        "print(counts, end=\"\\n\\n\")\n",
        "\n",
        "# (2) Constituency Parsing\n",
        "# Define a simple context-free grammar for constituency parsing\n",
        "cgrammar = nltk.CFG.fromstring(\"\"\"\n",
        "    S -> NP VP\n",
        "    VP -> V NP\n",
        "    PP -> P NP\n",
        "    NP -> NP PP | Det N | 'Peter' | 'Denver'\n",
        "    V -> 'prefers'\n",
        "    P -> 'from'\n",
        "    N -> 'flight'\n",
        "    Det -> 'the'\n",
        "\"\"\")\n",
        "\n",
        "# Example sentence for constituency parsing\n",
        "sentence = ['Peter', 'prefers', 'the', 'flight', 'from', 'Denver']\n",
        "\n",
        "# Using Chart Parser for constituency parsing\n",
        "cparser = ChartParser(cgrammar)\n",
        "for tree in cparser.parse(sentence):\n",
        "    print(tree, end=\"\\n\")\n",
        "\n",
        "# Drawing the constituency parsing tree\n",
        "svgling.draw_tree(tree)\n",
        "\n",
        "# (3) Named Entity Recognition\n",
        "named_entities = []\n",
        "for sentence in df['Cleaned Text']:\n",
        "    doc = nlp(sentence)\n",
        "    for entity in doc.ents:\n",
        "        if entity.text and entity.label_:\n",
        "            named_entities.append((entity.text, entity.label_))\n",
        "\n",
        "# Count the occurrences of each named entity type\n",
        "entity_counts = Counter(entity[1] for entity in named_entities)\n",
        "print(named_entities, end=\"\\n\")\n",
        "print(entity_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "P1rO-HnMPyMi",
        "outputId": "d75eb89d-8e65-4034-f951-ac95ae1e1bbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Cleaned Text'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Cleaned Text'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-3e6f27490e67>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# (1) Parts of Speech (POS) Tagging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Tag Parts of Speech of each word in the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Cleaned Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Cleaned Text'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "The assignment was had few questions but this was too hard for me to apply various NLP techniques. The data colelctiona and the cleaning them felt challenging for me, mainly when handling the stopwords and third question regarding the Constituency Parsing and Dependency Parsing was something new to me and it took lot of time. The time given for this assignment was ok. overall, I had a challenging and practicing with real-world text data was so good."
      ],
      "metadata": {
        "id": "_e557s2w4BpK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}