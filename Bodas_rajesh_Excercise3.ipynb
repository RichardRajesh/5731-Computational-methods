{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RichardRajesh/5731-Computational-methods/blob/main/Bodas_rajesh_Excercise3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "7b5abddf-c6dc-41db-dec5-08fd94ff222a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n I have done textmining for the first time in last semister where in i have ciollected the data related \\nto job postings from the 'reddit' app. However i felt thetext mining i have done in my last assignment in this course was more intresting\\n where in i have collected data of movie reviews from the imdb website where i was asked to collect 1000 reviews from two movies. The\\nimportent features i beleve that could be helpful in building the model are\\n* The username of the reviewer which could be helpful in identifying the user details whose provile could help on analysing\\nthe strenth of the review provided by a person.\\n* Movie name obviously helps in differentiating the reviews by the film they belong to when we have a pool of movie reviews.\\n* Review Date helps in understanding since how long the movie is in craze in people.\\n* Url of the movie review data for better accessability\\n* Rating helps in umnderstanding the review easily as a number can explain it well.\\n* review text which is the actual input here to asses the movie.\\n\\n\\nOveral, above five features coulkd help in better analysing of the given data. and for extracting those features we can \\nuse techniques like text filtering, word count, Parts of speach, sentimental analysis etc.\\nWhen I'm using the or the building up the machine learning model for the purpose of the sentimental analysis when using the IMDB movie reviews, I need many types of feature. below is the explanation of each feature.Word Frequency Features: Examining the frequency with which particular words show up in reviews to gauge the general tone.\\nN-grams Features: Analyzing word combinations within the text to extract subtleties of sentiment and context.\\nemotion Lexicon Features: Assessing emotion in reviews by looking up lists of terms with corresponding sentiment polarity.\\nPart-of-Speech (POS) Tagging Features: Classifying words according to their grammar in order to spot sentiment-indicating patterns.\\nSentence Structure Features: Sentiment expression may be gleaned by examining sentence length and structure.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        " I have done textmining for the first time in last semister where in i have ciollected the data related\n",
        "to job postings from the 'reddit' app. However i felt thetext mining i have done in my last assignment in this course was more intresting\n",
        " where in i have collected data of movie reviews from the imdb website where i was asked to collect 1000 reviews from two movies. The\n",
        "importent features i beleve that could be helpful in building the model are\n",
        "* The username of the reviewer which could be helpful in identifying the user details whose provile could help on analysing\n",
        "the strenth of the review provided by a person.\n",
        "* Movie name obviously helps in differentiating the reviews by the film they belong to when we have a pool of movie reviews.\n",
        "* Review Date helps in understanding since how long the movie is in craze in people.\n",
        "* Url of the movie review data for better accessability\n",
        "* Rating helps in umnderstanding the review easily as a number can explain it well.\n",
        "* review text which is the actual input here to asses the movie.\n",
        "\n",
        "\n",
        "Overal, above five features coulkd help in better analysing of the given data. and for extracting those features we can\n",
        "use techniques like text filtering, word count, Parts of speach, sentimental analysis etc.\n",
        "When I'm using the or the building up the machine learning model for the purpose of the sentimental analysis when using the IMDB movie reviews, I need many types of feature. below is the explanation of each feature.Word Frequency Features: Examining the frequency with which particular words show up in reviews to gauge the general tone.\n",
        "N-grams Features: Analyzing word combinations within the text to extract subtleties of sentiment and context.\n",
        "emotion Lexicon Features: Assessing emotion in reviews by looking up lists of terms with corresponding sentiment polarity.\n",
        "Part-of-Speech (POS) Tagging Features: Classifying words according to their grammar in order to spot sentiment-indicating patterns.\n",
        "Sentence Structure Features: Sentiment expression may be gleaned by examining sentence length and structure.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X21vDsEqfmxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def extracted_reviews_text(url_link):\n",
        "    responses = requests.get(url_link)\n",
        "    if responses.status_code == 200:\n",
        "        soup = BeautifulSoup(responses.text, 'html.parser')\n",
        "        reviews_containers = soup.find_all('div', class_='lister-item-content')[:5]\n",
        "        return reviews_containers\n",
        "    else:\n",
        "        print(\"Failed to retrieve data from IMDb.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def extracted_features_data(reviews_containers, movies_name_text, movie_url_link):\n",
        "    extracted_text_data = []\n",
        "    for reviews_items in reviews_containers:\n",
        "\n",
        "        reviewer_usernames = reviews_items.find('span', class_='display-name-link').text.strip()\n",
        "\n",
        "\n",
        "        reviews_date = reviews_items.find('span', class_='review-date').text.strip()\n",
        "\n",
        "\n",
        "        reviews_ratings_tag = reviews_items.find('span', class_='rating-other-user-rating')\n",
        "        reviews_ratings = reviews_ratings_tag.text.strip() if reviews_ratings_tag else None\n",
        "\n",
        "\n",
        "        reviews_text_data = reviews_items.find('div', class_='text show-more__control').text.strip()\n",
        "\n",
        "\n",
        "        extracted_text_data.append([movies_name_text, reviewer_usernames, reviews_date, reviews_text_data, movie_url_link, reviews_ratings])\n",
        "\n",
        "    return extracted_text_data\n",
        "\n",
        "\n",
        "movies_links = [\n",
        "    (\"https://www.imdb.com/title/tt2872732/reviews/?ref_=tt_ov_rt\", \"Jurassic Park\"),\n",
        "    (\"https://www.imdb.com/title/tt0107290/reviews/?ref_=tt_ov_rt\", \"The Lion King\")\n",
        "]\n",
        "\n",
        "\n",
        "extracted_reviews_text_data = []\n",
        "\n",
        "\n",
        "for movie_url_link, movies_name_text in movies_links:\n",
        "    reviews_containers = extracted_reviews_text(movie_url_link)\n",
        "    if reviews_containers:\n",
        "        extracted_reviews_text_data.extend(extracted_features_data(reviews_containers, movies_name_text, movie_url_link))\n",
        "\n",
        "\n",
        "df_reviews_data = pd.DataFrame(extracted_reviews_text_data, columns=[\"Movie Name\", \"Reviewer Username\", \"Review Date\", \"Review Text\", \"URL\", \"Rating\"])\n",
        "\n",
        "\n",
        "print(df_reviews_data)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d1h90_yDott",
        "outputId": "993c7a28-dabd-4f7a-c655-9cb2c6b3336a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Movie Name   Reviewer Username       Review Date  \\\n",
            "0  Jurassic Park          dilloncamp       29 May 2021   \n",
            "1  Jurassic Park          swordsnare     22 March 2020   \n",
            "2  Jurassic Park         gogoschka-1     4 August 2014   \n",
            "3  Jurassic Park        latinfineart   4 February 2018   \n",
            "4  Jurassic Park           ma-cortes    12 August 2022   \n",
            "5  The Lion King  Pjtaylor-96-138044      10 June 2018   \n",
            "6  The Lion King       planktonrules       11 May 2013   \n",
            "7  The Lion King  FrankBooth_DeLarge  5 September 2005   \n",
            "8  The Lion King  Smells_Like_Cheese  18 November 2003   \n",
            "9  The Lion King   TheLittleSongbird     30 March 2010   \n",
            "\n",
            "                                         Review Text  \\\n",
            "0  This film was a wild ride. And yes it's not sc...   \n",
            "1  I enjoyed this for the most-part. The momentum...   \n",
            "2  After the many scathing reviews, I went to thi...   \n",
            "3  Unless you are nearly brain dead, you will fin...   \n",
            "4  This impressive comic-book style tale brought ...   \n",
            "5  'Jurassic Park (1993)' is a landmark achieveme...   \n",
            "6  By now there are nearly a thousand (not really...   \n",
            "7  In the year 1993, Hollywood saw something unli...   \n",
            "8  My first epic movie to see on the big screen w...   \n",
            "9  I personally really like Steven Spielberg, and...   \n",
            "\n",
            "                                                 URL Rating  \n",
            "0  https://www.imdb.com/title/tt2872732/reviews/?...   8/10  \n",
            "1  https://www.imdb.com/title/tt2872732/reviews/?...   7/10  \n",
            "2  https://www.imdb.com/title/tt2872732/reviews/?...   None  \n",
            "3  https://www.imdb.com/title/tt2872732/reviews/?...   9/10  \n",
            "4  https://www.imdb.com/title/tt2872732/reviews/?...   6/10  \n",
            "5  https://www.imdb.com/title/tt0107290/reviews/?...  10/10  \n",
            "6  https://www.imdb.com/title/tt0107290/reviews/?...   9/10  \n",
            "7  https://www.imdb.com/title/tt0107290/reviews/?...  10/10  \n",
            "8  https://www.imdb.com/title/tt0107290/reviews/?...   9/10  \n",
            "9  https://www.imdb.com/title/tt0107290/reviews/?...   9/10  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "def extracted_n_grams(text, n):\n",
        "    token_values = word_tokenize(text.lower())\n",
        "    n_gram_values = ngrams(token_values, n)\n",
        "    return Counter(n_gram_values)\n",
        "\n",
        "text_data = \"I have done textmining for the first time in last semister where in i have ciollected the data related \\nto job postings from the 'reddit' app. However i felt thetext mining i have done in my last assignment in this course was more intresting\\n where in i have collected data of movie reviews from the imdb website where i was asked to collect 1000 reviews from two movies. The\\nimportent features i beleve that could be helpful in building the model are\\n* The username of the reviewer which could be helpful in identifying the user details whose provile could help on analysing\\nthe strenth of the review provided by a person.\\n* Movie name obviously helps in differentiating the reviews by the film they belong to when we have a pool of movie reviews.\\n* Review Date helps in understanding since how long the movie is in craze in people.\\n* Url of the movie review data for better accessability\\n* Rating helps in umnderstanding the review easily as a number can explain it well.\"\n",
        "n = 2\n",
        "ngrams_features = extracted_n_grams(text_data, n)\n",
        "print(ngrams_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2pMZneAey8f",
        "outputId": "1ab96a96-0247-436a-b769-337e819abe68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({('i', 'have'): 4, ('of', 'the'): 3, ('.', '*'): 3, ('helps', 'in'): 3, ('have', 'done'): 2, ('where', 'in'): 2, ('in', 'i'): 2, ('from', 'the'): 2, ('of', 'movie'): 2, ('movie', 'reviews'): 2, ('reviews', 'from'): 2, ('could', 'be'): 2, ('be', 'helpful'): 2, ('helpful', 'in'): 2, ('the', 'review'): 2, ('the', 'movie'): 2, ('done', 'textmining'): 1, ('textmining', 'for'): 1, ('for', 'the'): 1, ('the', 'first'): 1, ('first', 'time'): 1, ('time', 'in'): 1, ('in', 'last'): 1, ('last', 'semister'): 1, ('semister', 'where'): 1, ('have', 'ciollected'): 1, ('ciollected', 'the'): 1, ('the', 'data'): 1, ('data', 'related'): 1, ('related', 'to'): 1, ('to', 'job'): 1, ('job', 'postings'): 1, ('postings', 'from'): 1, ('the', \"'reddit\"): 1, (\"'reddit\", \"'\"): 1, (\"'\", 'app'): 1, ('app', '.'): 1, ('.', 'however'): 1, ('however', 'i'): 1, ('i', 'felt'): 1, ('felt', 'thetext'): 1, ('thetext', 'mining'): 1, ('mining', 'i'): 1, ('done', 'in'): 1, ('in', 'my'): 1, ('my', 'last'): 1, ('last', 'assignment'): 1, ('assignment', 'in'): 1, ('in', 'this'): 1, ('this', 'course'): 1, ('course', 'was'): 1, ('was', 'more'): 1, ('more', 'intresting'): 1, ('intresting', 'where'): 1, ('have', 'collected'): 1, ('collected', 'data'): 1, ('data', 'of'): 1, ('the', 'imdb'): 1, ('imdb', 'website'): 1, ('website', 'where'): 1, ('where', 'i'): 1, ('i', 'was'): 1, ('was', 'asked'): 1, ('asked', 'to'): 1, ('to', 'collect'): 1, ('collect', '1000'): 1, ('1000', 'reviews'): 1, ('from', 'two'): 1, ('two', 'movies'): 1, ('movies', '.'): 1, ('.', 'the'): 1, ('the', 'importent'): 1, ('importent', 'features'): 1, ('features', 'i'): 1, ('i', 'beleve'): 1, ('beleve', 'that'): 1, ('that', 'could'): 1, ('in', 'building'): 1, ('building', 'the'): 1, ('the', 'model'): 1, ('model', 'are'): 1, ('are', '*'): 1, ('*', 'the'): 1, ('the', 'username'): 1, ('username', 'of'): 1, ('the', 'reviewer'): 1, ('reviewer', 'which'): 1, ('which', 'could'): 1, ('in', 'identifying'): 1, ('identifying', 'the'): 1, ('the', 'user'): 1, ('user', 'details'): 1, ('details', 'whose'): 1, ('whose', 'provile'): 1, ('provile', 'could'): 1, ('could', 'help'): 1, ('help', 'on'): 1, ('on', 'analysing'): 1, ('analysing', 'the'): 1, ('the', 'strenth'): 1, ('strenth', 'of'): 1, ('review', 'provided'): 1, ('provided', 'by'): 1, ('by', 'a'): 1, ('a', 'person'): 1, ('person', '.'): 1, ('*', 'movie'): 1, ('movie', 'name'): 1, ('name', 'obviously'): 1, ('obviously', 'helps'): 1, ('in', 'differentiating'): 1, ('differentiating', 'the'): 1, ('the', 'reviews'): 1, ('reviews', 'by'): 1, ('by', 'the'): 1, ('the', 'film'): 1, ('film', 'they'): 1, ('they', 'belong'): 1, ('belong', 'to'): 1, ('to', 'when'): 1, ('when', 'we'): 1, ('we', 'have'): 1, ('have', 'a'): 1, ('a', 'pool'): 1, ('pool', 'of'): 1, ('reviews', '.'): 1, ('*', 'review'): 1, ('review', 'date'): 1, ('date', 'helps'): 1, ('in', 'understanding'): 1, ('understanding', 'since'): 1, ('since', 'how'): 1, ('how', 'long'): 1, ('long', 'the'): 1, ('movie', 'is'): 1, ('is', 'in'): 1, ('in', 'craze'): 1, ('craze', 'in'): 1, ('in', 'people'): 1, ('people', '.'): 1, ('*', 'url'): 1, ('url', 'of'): 1, ('movie', 'review'): 1, ('review', 'data'): 1, ('data', 'for'): 1, ('for', 'better'): 1, ('better', 'accessability'): 1, ('accessability', '*'): 1, ('*', 'rating'): 1, ('rating', 'helps'): 1, ('in', 'umnderstanding'): 1, ('umnderstanding', 'the'): 1, ('review', 'easily'): 1, ('easily', 'as'): 1, ('as', 'a'): 1, ('a', 'number'): 1, ('number', 'can'): 1, ('can', 'explain'): 1, ('explain', 'it'): 1, ('it', 'well'): 1, ('well', '.'): 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def extract_emotional_lexicon(text_data):\n",
        "    textblob = TextBlob(text_data)\n",
        "    return textblob.sentiment\n",
        "\n",
        "text_data = \"I have done textmining for the first time in last semister where in i have ciollected the data related \\nto job postings from the 'reddit' app. However i felt thetext mining i have done in my last assignment in this course was more intresting\\n where in i have collected data of movie reviews from the imdb website where i was asked to collect 1000 reviews from two movies. The\\nimportent features i beleve that could be helpful in building the model are\\n* The username of the reviewer which could be helpful in identifying the user details whose provile could help on analysing\\nthe strenth of the review provided by a person.\\n* Movie name obviously helps in differentiating the reviews by the film they belong to when we have a pool of movie reviews.\\n* Review Date helps in understanding since how long the movie is in craze in people.\\n* Url of the movie review data for better accessability\\n* Rating helps in umnderstanding the review easily as a number can explain it well.\"\n",
        "emotionala_features = extract_emotional_lexicon(text_data)\n",
        "print(emotionala_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3wb8HGyfqgD",
        "outputId": "17a90ca8-0555-4e43-c277-b52dbef42bd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment(polarity=0.18148148148148147, subjectivity=0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "def extract_POS_tags(text):\n",
        "    tokens = nltk.word_tokenize(text_data)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    return pos_tags\n",
        "\n",
        "text_data = \"I have done textmining for the first time in last semister where in i have ciollected the data related \\nto job postings from the 'reddit' app. However i felt thetext mining i have done in my last assignment in this course was more intresting\\n where in i have collected data of movie reviews from the imdb website where i was asked to collect 1000 reviews from two movies. The\\nimportent features i beleve that could be helpful in building the model are\\n* The username of the reviewer which could be helpful in identifying the user details whose provile could help on analysing\\nthe strenth of the review provided by a person.\\n* Movie name obviously helps in differentiating the reviews by the film they belong to when we have a pool of movie reviews.\\n* Review Date helps in understanding since how long the movie is in craze in people.\\n* Url of the movie review data for better accessability\\n* Rating helps in umnderstanding the review easily as a number can explain it well.\"\n",
        "pos_tag_data = extract_POS_tags(text_data)\n",
        "print(pos_tag_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5HgTmr6f9WG",
        "outputId": "7a31713e-e45c-419f-e48f-b41fb12c4253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP'), ('have', 'VBP'), ('done', 'VBN'), ('textmining', 'NN'), ('for', 'IN'), ('the', 'DT'), ('first', 'JJ'), ('time', 'NN'), ('in', 'IN'), ('last', 'JJ'), ('semister', 'NN'), ('where', 'WRB'), ('in', 'IN'), ('i', 'NN'), ('have', 'VBP'), ('ciollected', 'VBN'), ('the', 'DT'), ('data', 'NNS'), ('related', 'VBN'), ('to', 'TO'), ('job', 'NN'), ('postings', 'NNS'), ('from', 'IN'), ('the', 'DT'), (\"'reddit\", 'NN'), (\"'\", 'POS'), ('app', 'NN'), ('.', '.'), ('However', 'RB'), ('i', 'JJ'), ('felt', 'VBD'), ('thetext', 'JJ'), ('mining', 'NN'), ('i', 'NN'), ('have', 'VBP'), ('done', 'VBN'), ('in', 'IN'), ('my', 'PRP$'), ('last', 'JJ'), ('assignment', 'NN'), ('in', 'IN'), ('this', 'DT'), ('course', 'NN'), ('was', 'VBD'), ('more', 'RBR'), ('intresting', 'JJ'), ('where', 'WRB'), ('in', 'IN'), ('i', 'NN'), ('have', 'VBP'), ('collected', 'VBN'), ('data', 'NNS'), ('of', 'IN'), ('movie', 'NN'), ('reviews', 'NNS'), ('from', 'IN'), ('the', 'DT'), ('imdb', 'NN'), ('website', 'NN'), ('where', 'WRB'), ('i', 'NN'), ('was', 'VBD'), ('asked', 'VBN'), ('to', 'TO'), ('collect', 'VB'), ('1000', 'CD'), ('reviews', 'NNS'), ('from', 'IN'), ('two', 'CD'), ('movies', 'NNS'), ('.', '.'), ('The', 'DT'), ('importent', 'NN'), ('features', 'VBZ'), ('i', 'JJ'), ('beleve', 'VBP'), ('that', 'DT'), ('could', 'MD'), ('be', 'VB'), ('helpful', 'JJ'), ('in', 'IN'), ('building', 'VBG'), ('the', 'DT'), ('model', 'NN'), ('are', 'VBP'), ('*', 'PDT'), ('The', 'DT'), ('username', 'NN'), ('of', 'IN'), ('the', 'DT'), ('reviewer', 'NN'), ('which', 'WDT'), ('could', 'MD'), ('be', 'VB'), ('helpful', 'JJ'), ('in', 'IN'), ('identifying', 'VBG'), ('the', 'DT'), ('user', 'NN'), ('details', 'NNS'), ('whose', 'WP$'), ('provile', 'NN'), ('could', 'MD'), ('help', 'VB'), ('on', 'IN'), ('analysing', 'VBG'), ('the', 'DT'), ('strenth', 'NN'), ('of', 'IN'), ('the', 'DT'), ('review', 'NN'), ('provided', 'VBN'), ('by', 'IN'), ('a', 'DT'), ('person', 'NN'), ('.', '.'), ('*', 'JJ'), ('Movie', 'NNP'), ('name', 'NN'), ('obviously', 'RB'), ('helps', 'VBZ'), ('in', 'IN'), ('differentiating', 'VBG'), ('the', 'DT'), ('reviews', 'NNS'), ('by', 'IN'), ('the', 'DT'), ('film', 'NN'), ('they', 'PRP'), ('belong', 'VBP'), ('to', 'TO'), ('when', 'WRB'), ('we', 'PRP'), ('have', 'VBP'), ('a', 'DT'), ('pool', 'NN'), ('of', 'IN'), ('movie', 'NN'), ('reviews', 'NNS'), ('.', '.'), ('*', 'VB'), ('Review', 'NNP'), ('Date', 'NNP'), ('helps', 'VBZ'), ('in', 'IN'), ('understanding', 'VBG'), ('since', 'IN'), ('how', 'WRB'), ('long', 'JJ'), ('the', 'DT'), ('movie', 'NN'), ('is', 'VBZ'), ('in', 'IN'), ('craze', 'NN'), ('in', 'IN'), ('people', 'NNS'), ('.', '.'), ('*', 'JJ'), ('Url', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('movie', 'NN'), ('review', 'NN'), ('data', 'NNS'), ('for', 'IN'), ('better', 'JJR'), ('accessability', 'NN'), ('*', 'NN'), ('Rating', 'NNP'), ('helps', 'VBZ'), ('in', 'IN'), ('umnderstanding', 'VBG'), ('the', 'DT'), ('review', 'NN'), ('easily', 'RB'), ('as', 'IN'), ('a', 'DT'), ('number', 'NN'), ('can', 'MD'), ('explain', 'VB'), ('it', 'PRP'), ('well', 'RB'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "n_l_p = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_sentence_structure_features(text_data):\n",
        "    doc = n_l_p(text_data)\n",
        "    return [sent.root.pos_ for sent in doc.sents]\n",
        "\n",
        "text_data = \"I have done textmining for the first time in last semister where in i have ciollected the data related \\nto job postings from the 'reddit' app. However i felt thetext mining i have done in my last assignment in this course was more intresting\\n where in i have collected data of movie reviews from the imdb website where i was asked to collect 1000 reviews from two movies. The\\nimportent features i beleve that could be helpful in building the model are\\n* The username of the reviewer which could be helpful in identifying the user details whose provile could help on analysing\\nthe strenth of the review provided by a person.\\n* Movie name obviously helps in differentiating the reviews by the film they belong to when we have a pool of movie reviews.\\n* Review Date helps in understanding since how long the movie is in craze in people.\\n* Url of the movie review data for better accessability\\n* Rating helps in umnderstanding the review easily as a number can explain it well.\"\n",
        "sentence_structure = extract_sentence_structure_features(text_data)\n",
        "print(sentence_structure)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eMDtUz1gEPu",
        "outputId": "8ee0da4d-73a1-4e1d-be38-1c8bcd44a041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['VERB', 'VERB', 'AUX', 'NOUN', 'PUNCT', 'VERB', 'VERB', 'NOUN', 'VERB']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "vectorizers = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "X = vectorizers.fit_transform(df_reviews_data['Review Text'])\n",
        "\n",
        "term_score_data = X.sum(axis=0)\n",
        "\n",
        "features = vectorizers.get_feature_names_out()\n",
        "\n",
        "df_term_scores_data = pd.DataFrame(term_score_data.T, index=features, columns=['TF-IDF Score'])\n",
        "\n",
        "rankedd_term = df_term_scores_data.sort_values(by='TF-IDF Score', ascending=False)\n",
        "\n",
        "print(\"Ranked terms based on TF-IDF scores:\")\n",
        "print(rankedd_term)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0yxGaGqDG6Q",
        "outputId": "a020405d-638d-434e-bdb0-25ad10dcba4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked terms based on TF-IDF scores:\n",
            "           TF-IDF Score\n",
            "film           0.872460\n",
            "park           0.739107\n",
            "lucy           0.635748\n",
            "dinosaurs      0.567218\n",
            "good           0.527319\n",
            "...                 ...\n",
            "composer       0.046709\n",
            "complex        0.046709\n",
            "company        0.046709\n",
            "colorful       0.046709\n",
            "latest         0.046709\n",
            "\n",
            "[1064 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Query\n",
        "query_input = \"The plot is very original.\"\n",
        "\n",
        "# Tokenize query\n",
        "tokens = bert_tokenizer.encode(query_input, add_special_tokens=True, return_tensors=\"pt\")\n",
        "\n",
        "# Generate embeddings for query\n",
        "with torch.no_grad():\n",
        "    query_outputs = bert_model(tokens)[0][:, 0, :].squeeze()\n",
        "\n",
        "# Calculate cosine similarity for each review in the text data\n",
        "ranked_reviews = []\n",
        "ranked_similarities = []\n",
        "for review_text in df_reviews_data['Review Text']:\n",
        "    # Truncate review text to fit within the maximum sequence length\n",
        "    review_text = review_text[:512]\n",
        "\n",
        "    # Tokenize review\n",
        "    reviews_tokens = bert_tokenizer.encode(review_text, add_special_tokens=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate embeddings for review\n",
        "    with torch.no_grad():\n",
        "        output_reviews = bert_model(reviews_tokens)[0][:, 0, :].squeeze()\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity_score = cosine_similarity(query_outputs.reshape(1, -1), output_reviews.reshape(1, -1))[0][0]\n",
        "\n",
        "    # Append similarity and review\n",
        "    ranked_similarities.append(similarity_score)\n",
        "    ranked_reviews.append(review_text)\n",
        "\n",
        "# Rank similarities in descending order\n",
        "ranked_indices = sorted(range(len(ranked_similarities)), key=lambda i: ranked_similarities[i], reverse=True)\n",
        "ranked_reviews = [ranked_reviews[i] for i in ranked_indices]\n",
        "ranked_similarities = [ranked_similarities[i] for i in ranked_indices]\n",
        "\n",
        "# Print ranked text data and similarities\n",
        "for i, (review, similarity) in enumerate(zip(ranked_reviews, ranked_similarities), 1):\n",
        "    print(f\"Rank {i}:\")\n",
        "    print(f\"Review: {review}\")\n",
        "    print(f\"Similarity: {similarity}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5OOirK8eUee",
        "outputId": "6b395133-fca0-4368-87dd-e7e07cadf191"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank 1:\n",
            "Review: I enjoyed this for the most-part. The momentum in combination with the 90 minute runtime, kept myself entertained for the entirety. It's far from perfect, but it's entertaining. Then again I didn't labour over the science to over complicate the overall enjoyment. The climax is probably the most contentious and unfavourable issue, but I appreciate the intent of Luc Besson's vision.Note: It's probably best to ignore the super low-scoring trolls, who create accounts to downvote and spread negativity. If you do\n",
            "Similarity: 0.8579081296920776\n",
            "\n",
            "Rank 2:\n",
            "Review: After the many scathing reviews, I went to this film with very mixed feelings. I certainly didn't expect serious Sci-Fi – after all, I had seen the trailers – but I actually wasn't quite sure what to expect (and I'm not sure I would have gone to see it at all, If it hadn't been directed by Luc Besson). Well, maybe it's just because I expected to be disappointed, but I was pleasantly surprised. 'Lucy' delivers a fast paced, crazy ride from start to finish, and I'm frankly a bit shocked so many people seem to\n",
            "Similarity: 0.8430196046829224\n",
            "\n",
            "Rank 3:\n",
            "Review: This film was a wild ride. And yes it's not scientifically accurate, but so what?It was an awesome idea that I found well executed.I will say I wanted to see ScarJo kick more ass but I still didn't mind. This isn't as much an action movie as it is a scifi thriller.It really makes you question your perception of science and reality.If your looking for a good scifi thriller that keeps you interested all the way through I highly suggest this film.\n",
            "Similarity: 0.837446928024292\n",
            "\n",
            "Rank 4:\n",
            "Review: By now there are nearly a thousand (not really a bazillion) reviews for this Steven Spielberg film. So, in the case of mega-hits where there is a strong consensus that the movie is exceptional (and I don't disagree), what more is there to say?! Apart from a few characters who seemed a bit one-dimensional (which is a minor problem in an action film) and the unwritten Spielberg rule that kids cannot die (taking away much of the suspense), the film is amazing. While the CGI isn't quite as beautiful as what we \n",
            "Similarity: 0.8231382966041565\n",
            "\n",
            "Rank 5:\n",
            "Review: In the year 1993, Hollywood saw something unlike anything else, the film Jurassic Park. Never before 1993 had dinosaurs been so breathtaking and realistic on the silver screen.The plot is very original. A group of scientists cloned dinosaurs, and are about to open an amusement park where people can see the dinosaurs. The creator John Hammond(Richard Attenborough) invites a group of people, along with his grandchildren, to see the dinos and enjoy a relaxing time at the park. Could anything go wrong, at the t\n",
            "Similarity: 0.807144820690155\n",
            "\n",
            "Rank 6:\n",
            "Review: 'Jurassic Park (1993)' is a landmark achievement, in as many ways as possible. While some of the exclusively digital dinosaurs don't hold up quite as well as the rest (despite having been only relatively recently surpassed by their peers - depending on budgetary constraints, of course), there are times when the superb CG almost seamlessly blends with possibly the best practical effects I've ever seen to create phenomenally believable, groundbreaking special-effects that truly are, still to this day, some of\n",
            "Similarity: 0.7887148857116699\n",
            "\n",
            "Rank 7:\n",
            "Review: Unless you are nearly brain dead, you will find this film stimulating. If you are a sci-fi buff like me, even more so. A film like this reminds me of how starved some of us are for films of this sort. This film was brilliantly directed by the French Maestro. Beautifully acted, by Scarlotte Johansson. While the transformation is happening, we see Professor Norman (Morgan Freeman) giving a lecture on the capacity of the human brain. He says people generally use only about 10 percent of their brain's capacity.\n",
            "Similarity: 0.778867244720459\n",
            "\n",
            "Rank 8:\n",
            "Review: I personally really like Steven Spielberg, and I like his films, especially Schindlers List, Jaws and ET:The Extra Terrestrial. While a tad too long and has one or two loose ends in the plot, Jurassic Park nevertheless has thrilling action and has sheer evidence of the director's vision and effortless verve at work. John Williams's score is absolutely terrific, definitely one of the more memorable scores in a Steven Spielberg movie, the score for Jaws like Psycho is one of the main reason why I am so scared\n",
            "Similarity: 0.7400071620941162\n",
            "\n",
            "Rank 9:\n",
            "Review: This impressive comic-book style tale brought to big screen contains thought-provoking science-fiction , fantasy , noisy action , battles , thrills , violent fights and results to be pretty entertaining . Lucy (Scarlett Johansson) is a smart , feisty college student who suddenly finds herself involved into a horrific drug and human trafficking situation . Later on , her brain is permanently altered when the fantastic drug CPH4 is released in her body . Lucy being re-animated due to the incredible synthetic \n",
            "Similarity: 0.6485110521316528\n",
            "\n",
            "Rank 10:\n",
            "Review: My first epic movie to see on the big screen was Jurassic Park. I was only 8 years old and I had no idea what I was in for when I watched this movie. I was terrified but so enchanted by this world. Steven Spielberg brought back the dinosaur genre, a genre that was only used with caveman movies. Not to mention did it with incredible special effects. I remember seeing this movie for the first time and asked my mom if the dinosaurs were real or if they made giant robots. Those dinosaurs not only looked real bu\n",
            "Similarity: 0.6182072758674622\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "* The overall learning experience was so intresting and challenging as well as the time is limited and there is a lot to look at.\n",
        "The keytake awys i found more benifecial are understanding the feature selection and feature extraction concepts, the refrence\n",
        "paper provided has helped a lot in understanding every aspect of text mining.\n",
        "* The difficult part is ranking the reviews with respect to the similarity i mean there are various techniques and its hard to\n",
        "get them all at once.\n",
        "*Ofcourse its very much related to NLP as these are the steps that extracts the meaningfull content from the pool of data that\n",
        "acts as an input to NLP architecture more often.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}